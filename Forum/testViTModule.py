import os
import torch
from PIL import Image
from transformers import AutoModelForImageClassification, ViTImageProcessor

# ==============================
# LOAD MODEL (chá»‰ load 1 láº§n)
# ==============================
print("ğŸ”„ Äang táº£i model NSFW...")
model = AutoModelForImageClassification.from_pretrained("Falconsai/nsfw_image_detection")
processor = ViTImageProcessor.from_pretrained("Falconsai/nsfw_image_detection")
print("âœ… Model loaded thÃ nh cÃ´ng!")

# ==============================
# HÃ€M CHECK NSFW
# ==============================
def check_nsfw(image_path):
    if not os.path.exists(image_path):
        print("âŒ áº¢nh khÃ´ng tá»“n táº¡i:", image_path)
        return

    try:
        img = Image.open(image_path).convert("RGB")
    except:
        print("âŒ File khÃ´ng pháº£i áº£nh há»£p lá»‡:", image_path)
        return

    # Tiá»n xá»­ lÃ½ vÃ  dá»± Ä‘oÃ¡n
    inputs = processor(images=img, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = logits.softmax(dim=-1)[0]

    # Káº¿t quáº£ dá»± Ä‘oÃ¡n
    labels = model.config.id2label
    pred_idx = logits.argmax(-1).item()
    pred_label = labels[pred_idx]

    print("\n===== ğŸ” Káº¿t quáº£ phÃ¢n loáº¡i =====")
    print(f"áº¢nh: {image_path}")
    print(f"ğŸ“Œ Label dá»± Ä‘oÃ¡n: {pred_label.upper()}")
    print(f"ğŸ“Š NSFW Score: {probs[1]:.4f}")
    print(f"ğŸ“Š NORMAL Score: {probs[0]:.4f}")

    if pred_label == "nsfw":
        print("ğŸš« Káº¾T LUáº¬N: áº¢NH KHÃ”NG AN TOÃ€N (NSFW) âŒ")
    else:
        print("ğŸŸ¢ Káº¾T LUáº¬N: áº¢NH AN TOÃ€N ğŸ‘")

# ==============================
# MAIN TEST
# ==============================
if __name__ == "__main__":
    image_path = "map.png"  # ğŸ‘‰ thay Ä‘á»•i tÃªn file táº¡i Ä‘Ã¢y
    check_nsfw(image_path)
